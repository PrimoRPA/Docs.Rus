# Настройка проекта для NLP-задач

Сразу после создания проекта вы автоматически попадете на страницу с настройкой проекта и тестированием навыков модели (две вкладки). Нет миллиона шагов, как в OCR.

(Скрин страницы с цифрами рабочих областей и поясняшками.)

Вкладка "Настройка" представляет собой рабочую область с тремя частями: 
* конфигурация проекта — выбор базовой NLP-модели и технических характеристик целевой машины.
* целевые машины — настройка одной или нескольких машин, на которой будет запущена ваша NLP-модель.
* навыки NLP-моделей — выбор навыков NLP-модели по отношению к тексту на естественном языке.


Суть обработки естественного языка в том, чтобы получать на вход текст, который нейронка обрабатывает, находит ключи/классифицирует текст/или создает на его основе текст

## Вкладка Настройка

### Конфигурация 1

Конфигурация состоит из основных и расширенных параметров.

Основные параметры:
1. **Выбор базовой модели** — выберите тип базовой модели, которая будет определять поведение модели нейросети. Доступные значения:
   * `llama3.1-8B (Vllm)` — базовая модель на сервере Vllm. Обрабатывает запросы быстрее. Работает не квантованная версия модели. Это более точная модель
   * `llama3.1-8B-GGUF (Llama)` — базовая модель на сервере Llama. Позволяет запускать большую языковую модель на низкотребовательном железе. Квантованная модель (?) с достаточно высоким квантованием. 

(на вкладке админа "Базовые модели" надо подробно описать, как создавать базовую NLP-модель)

1. **Ключ маршрутизации по умолчанию** — ключ по умолчанию для маршрутизации запросов. Ключ определяет, какой модели следует отправлять запрос для обработки. Ключ по умолчанию соответствует поведению модели без навыков. Модель ведет себя аналогично навыку "Генерация". Может быть не указан, если выбраны навыки?

Расширенные параметры:
1. **LLM-движок** — выберите тип движка. (хотели прикрутить к релизу связку движка и базовой модели - проверить потом). Доступные значения:
    * `Vllm` — сервер для базовой модели на Vllm.
    * `Llama` — сервер для базовой модели на Llama.
1. **Размер контекстного окна** — укажите объем текста, который модель рассматривает перед **генерацией** (и только?). По умолчанию `4096`.
1. **Адаптер** — не используется для NLP-задач, реализована для будущих версий. Пользовательская настройка для придания специфичности ответам базовой модели.
1. **Выбор устройства** — выберите тип устройства, который используется на целевой машине. Доступные значения:
    * `CPU` — по умолчанию.
    * `CUDA` —
   * **Кол-во используемых CPU** — параметр отображается, если выбрано устройство CPU. Укажите количество ядер CPU, которые будет использовать контейнер с выбранным движком для генерации ответов. По умолчанию `1` ядро.
   * **Кол-во используемых GPU** — параметр отображается, если выбрано устройство CUDA. Если модель не помещается в VRAM одной GPU, то следует использовать несколько видеокарт. По умолчанию `0`.
   * **Загруженность видеокарты** — параметр отображается, если выбрано устройство CUDA. Процент использования памяти видеокарты. Значение по умолчанию отсутствует.
   * **Кол-во памяти от хоста** — параметр отображается, если выбрано устройство CUDA. Если модель не помещается в VRAM одной GPU, часть слоев LLM оставляем на хосте.

Конфигурацию можно редактировать - только название - и удалить. После чего можно добавить новую конфигурацию.
Зачем меня имя конфигурации, например?

  
### Целевые машины

Выберите одну или несколько целевых машин для запуска NLP-модели. Целевая машина должна соответствовать выбранному устройству в конфигурации.

1. Нажмите **Добавить машину**.
1. Выберите название машины.
1. Переведите переключатель вправо, чтобы запустить NLP-модель на машине.
1. Убедитесь, что модель успешно запустилась на машине — напротив всех стадий запуска должна стоять галочка.


### Навыки

Нажмите **Добавить навык**....


## Тестирование
