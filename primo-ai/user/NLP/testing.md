# Тестирование навыков модели

Тестовые запросы позволяют без участия роботов Primo RPA оценить, как работают навыки модели. Запросы отправляются на вкладке **Тестирование**, а в ответах отображается результат, который получили бы роботы при отправке аналогичного запроса. Если ответ окажется неудовлетворительным, то с помощью параметров запроса вы сможете сконфигурировать навык так, чтобы повысить качество обработки текста. Помните, что использование данной страницы не является способом промышленной эксплутации модели — она предназначена только для отладки.

Вкладка **Тестирование** становится доступной после запуска модели на целевой машине. Рабочая область состоит из двух областей:
* Область для указания запроса
* Область для просмотра ответа — отображается после отправки запроса.

![](</primo-ai/resources/user/nlpproject/testing-empty.png>) 

## Параметры, которые влияют на ответ модели

**Ключи ответа.** Специфичные параметры, которые помогут модели лучше понять, что от нее требуется в ответе. Ключи указываются произвольно и зависят от поставленной NLP-задачи. Например, для навыка экстракции это могут быть ключи для поиска данных в тексте договора: сумма, дата и т.п.

**Системный промт**. Инструкция, которая позволяет на базовом уровне определить поведение модели: роль, задачи, язык и формат ответа. 

**Файл контекста.** Структурированный документ в формате json, который содержит набор примеров для техники few-shot prompting. Модель опирается на контекст при обработке запросов, чтобы лучше понимать, что от нее хотят и сгенерировать более точный ответ. Файл контекста должен быть индивидуальным для каждого навыка модели, поскольку примеры данных в запросах зависят от решаемой задачи.

Общая структура файла контекста (.json):
* Секция `"input"` — пример текста, который может быть отправлен в запросе для определенного навыка.
* Секция `"keys"` — ключи ответа или темы (в зависимости от наывка), которые помогут модели сосредоточиться на наиболее релевантной информации в тексте запроса.
* Секция `"output"` — пример ответа, который подскажет модели корректный вариант обработки текста, а также стиль изложения (более или менее формальный) и форматирование.

Таким образом, файл контекста помогает улучшать качество ответов модели и выдавать более релевантные ответы. В поставке Server AI имеется базовый комплект файлов контекста под каждый навык, который вы можете видоизменять. Дополнительно вы можете создавать собственные файлы, менять их для выбранного навыка или в рамках отдельно отправляемого запроса.

**Температура**. Управляет уровнем случайности в ответах модели, изменяя параметры распределения вероятности токенов. Более низкая температура приводит к более последовательным и предсказуемым результатам, в то время как более высокая температура увеличивает случайность и разнообразие генерируемых ответов.

Так, при низкой температуре модель склоняется к выбору наиболее вероятных токенов — основной единице текста в LLM, который не равен слову. Как правило, слово на русском — это приблизительно 2 токена. 

При более высокой температуре модель использует слова, которые менее часто встречаются в текстах, на которых она обучалась. 

**Минимальный порог**. Порог вероятности выбора токенов в ответе. Чем выше порог, тем наиболее вероятные токены модель возьмет в ответ, исключая из выборки редкие или неуместные токены. 

Параметры температуры и минимального порога работают в связке. Сначала отбирается диапазон вероятных токенов в соответствии с температурой, а затем токены дополнительно фильтруются по минимальному порогу. С их помощью вы можете снизить эффект *галлюцинаций* в ответах модели — когда ответ, который кажется на первый взгляд правдоподобным, на самом деле является неверным или бессмысленным.

**Длина ответа**. Предельное количество токенов, которые модель может генерировать при ответе. Длина может указываться позапросно. За использование токенов не взимается дополнительная оплата.

## Запрос

Чтобы отправить тестовый запрос, укажите следующие параметры:
1. **Текст для обработки\*** — входной текст для обработки. На текущий момент отсутствует возможность прикреплять файл документа. Максимальная длина текста регулируется параметром **Размер контекстного окна**, который вы установили при конфигурации модели.
1. **Ключи ответа\*** — специфичные для каждой NLP-задачи параметры. Если ключей несколько, указывайте их через запятую и избегайте ошибок.
1. **Ключ маршрутизации\*** — выберите ключ маршрутизации запроса, который соответствует тестируемому навыку модели. Ключи маршрутизации для каждого навыка вы указывали в разделе конфигурации модели.
1. **Длина ответа\*** — максимальная длина ответа модели в токенах. Для русского языка 1 слово ~ 2 токена. По умолчанию установлено значение `256`. 

Расширенные параметры:
1. **Температура** — значение параметра влияет на креативность ответа модели. По умолчанию `0.1` — оптимальное значение для корректных результатов.
1. **min_p** — дополнительный параметр, который определяет порог выбора токенов в ответе, где `0` — любой токен, `1` — самый вероятный. По умолчанию `0.1` — оптимальное значение для корректных результатов.
1. **Файл с контекстом (.json)** — возможность переопределить файл контекста для ключа маршрутизации. Например, добавить в файл дополнительные примеры данных и посмотреть, как будет изменяться поведение модели.
1. **Системный промт** — возможность переопределить поведение модели с указанным навыком на системном уровне. Например, изменить язык ответа (в этом случае язык также должен быть изменен в файле контекста).

В завершение нажмите **Отправить**, чтобы получить ответ от модели.

## Ответ

Ответ модели отобразится рядом с запросом — данные ответа можно скопировать или обновить. 

Ответ может быть в текстовом формате, например, для навыка генерации, или в виде json — например, для экстракции, когда информация извлекается по заданным ключам ответа. Если по указанному ключу модель нашла несколько значений, она отобразит все значения для данного ключа.

![](</primo-ai/resources/user/nlpproject/testing.png>) 

**Как уменьшить количество ошибок в ответах модели?**

В ситуациях, когда вам требуется более точный и логичный ответ, используйте следующие способы:
* Прописывайте все ключи ответа, а также лучше формулируйте запрос при использовании навыка генерации. Чем больше подробностей, тем лучше модель поймет, что от неё хотят. 
* Формируйте файлы контекста под задачи организации. Когда модель отвечает с опорой на составленный контекст, она может использовать эту информацию для более точных ответов. Укажите в файле контекста как можно больше информации, которая ей может пригодится.
* Ограничьте свободу модели. Отрегулируйте параметры температуры и минимального порога так, чтобы уменьшить шанс неправильного или выдуманного ответа. Например, для снижения галлюцинаций установите температуру `0.1` и повысьте значение в минимальном пороге.


## Что дальше
Следующий шаг — подготовить RPA-проект в Primo RPA Studio, который будет взаимодействовать с Primo RPA AI Server и машинами с LLM-моделью. 

Для разработки RPA-проекта установите в Primo RPA Studio библиотеку **Primo.AI.Server**:
* [Primo.AI.Server](https://docs.primo-rpa.ru/primo-rpa/g_elements/el_extra/ai_server) — описание пакета для Primo RPA Studio под Windows.



