# Тестирование навыков модели

Тестовые запросы позволяют быстро проверить, как работает выбранный навык модели без участия роботов Primo RPA. Запросы отправляются на вкладке **Тестирование** и фактически позволяют увидеть, что получит робот при отправке того или NLP-запроса. Если ответ окажется неудовлетворительным, то с помощью параметров запроса и файла контекста вы сможете сконфигурировать навык, чтобы повысить качество обработки текста. Помните, что использование данной страницы не является способом промышленной эксплутации модели — она предназначена для отладки навыков.

Вкладка **Тестирование** становится доступной после запуска модели на целевой машине — только тогда она готова принимать запросы и генерировать ответы в соответствии с навыком и задачей. 

Рабочая область состоит из двух областей:
* Область для указания запроса
* Область для просмотра ответа — отображается после отправки запроса.

![](</primo-ai/resources/user/nlpproject/testing-empty.png>) 

## Параметры, которые влияют на ответ модели

**Ключи ответа.** Специфичные параметры, которые помогут модели лучше понять, что от нее требуется в ответе. Ключи указываются произвольно и зависят от поставленной NLP-задачи. Например, для навыка экстракции это могут быть ключи для поиска данных в тексте договора: сумма, дата и т.п..

**Файл контекста.** Структурированный документ в формате json, который содержит набор данных для дополнительного контекста. Модель опирается на контекст при обработке запросов, чтобы лучше понимать, что от нее хотят и сгенерировать более точный ответ. Файл контекста должен быть индивидуальным для каждого навыка модели, поскольку примеры данных в запросах зависят от решаемой задачи.

Общая структура файла контекста (.json):
* Секция `"input"` — пример текста, который может быть отправлен в запросе для определенного навыка.
* Секция `"keys"` — ключевые слова, которые помогут модели сосредоточиться на наиболее релевантной информации в тексте запроса.
* Секция `"output"` — пример ответа, который подскажет модели корректный вариант обработки текста, а также стиль изложения (более или менее формальный).

Таким образом, файл контекста помогает улучшать качество ответов модели и выдавать более релевантные ответы. В поставке Server AI имеется базовый комплект файлов контекста под каждый навык, который вы можете видоизменять. Дополнительно вы можете создавать собственные файлы, менять их для выбранного навыка или в рамках отдельно отправляемого запроса.

**Температура**. Температура управляет уровнем случайности в ответах модели. Более низкая температура дает последовательные и предсказуемые результаты, а более высокая температура повышает случайность при генерации ответа. 

Так, при низкой температуре модель склоняется к выбору наиболее вероятных токенов — основной единице текста в LLM, который не равен слову. Как правило, слово на русском — это приблизительно 1.5-2 токена. 
При более высокой температуре модель использует слова, которые менее часто встречаются в текстах, на которых она обучалась. 

**Минимальный порог**. Порог вероятности выбора токенов в ответе. Чем выше порог, тем наиболее вероятные токены модель возьмет в ответ, исключая из выборки редкие или неуместные токены. 

Параметры температуры и минимального порога работают в связке. Сначала отбирается диапазон вероятных токенов в соответствии с температурой, а затем токены дополнительно фильтруются по минимальному порогу. С их помощью вы можете снизить эффект *галлюцинаций* в ответах модели — когда ответ, который кажется на первый взгляд правдоподобным, на самом деле является неверным или бессмысленным.

**Длина ответа**. Предельное количество токенов, которые модель может создать при ответе. За использование токенов не взимается дополнительная оплата.

## Запрос

Чтобы отправить тестовый запрос, укажите следующие параметры:
1. **Текст для обработки\*** — входной текст для обработки. На текущий момент отсутствует возможность прикреплять файл документа. Максимальная длина текста регулируется параметром **Размер контекстного окна**, который вы установили при конфигурации модели.
1. **Ключи ответа\*** — специфичные для каждой NLP-задачи параметры. Если ключей несколько, указывайте их через запятую и избегайте ошибок.
1. **Ключ маршрутизации\*** — выберите ключ маршрутизации запроса, который соответствует тестируемому навыку модели. Ключи маршрутизации для каждого навыка вы указывали в разделе конфигурации модели.
1. **Длина ответа\*** — максимальная длина ответа модели в токенах. Для русского языка 1 слово ~ 1.5-2 токена. По умолчанию установлено значение `256`. 

Расширенные параметры:
1. **Температура** — значение параметра влияет на креативность ответа модели. По умолчанию `0.1` — оптимальное значение для корректных результатов.
1. **min_p** — дополнительный параметр, который определяет порог выбора токенов в ответе, где `0` - любой токен, `1` - самый вероятный. По умолчанию `0.1` — оптимальное значение для корректных результатов.
1. **Файл с контекстом (.json)** — возможность переопределить файл контекста для ключа маршрутизации. Например, добавить в файл дополнительные примеры данных и посмотреть, как будет изменяться поведение модели.

В завершение нажмите **Отправить**, чтобы получить ответ от модели.

## Ответ

Область для получения ответа от NLP-модели. Ответ можно скопировать и обновить. 

Ошибки будут подсвечены - появится всплывашка. Добавить пример.

Ответ может быть выдан в виде текста - например, для навыка генерации. Или в виде json (структурированной информции) - например, для навыка экстракции, где будет извлекаться информация по ключам ответа.

Если по указанному ключу модель нашла несколько значений, она укажет их в значениях ключа.




![](</primo-ai/resources/user/nlpproject/testing.png>) 


Как уменьшить количество ошибок в ответах модели?

В ситуациях, когда вам требуется более точный и логичный ответ, используйте следующие способы:
Прописывайте ключи ответа, а также лучше формулируйте запрос при использовании навыка генерации. Чем больше подробностей, тем лучше модель поймет, что от неё хотят. 
Редактируйте файлы контекста под задачи своей организации. Когда модель отвечает с опорой на составленный контекст, она может использовать эту информацию для более точных ответов. Укажите в файле контекста как можно больше информации, которая ей может пригодится.
Ограничьте свободу модели. Отрегулируйте параметры температуры и минимального порога так, чтобы уменьшить шанс неправильного или выдуманного ответа. 

Следует понимать, что даже при этих условиях полностью исключить неправильные ответы модели не получится. 


   каждый следующий токен имеет некую вероятность появления. Она может быть 30%, 90%. И когда явная информация - модель ее найдет. А если, например, "расскажи мне стих", то будет много вероятностей. Параметр определяет, какой уровень отсечки - от самого высоко вероятного токена до самого низкого модель будет брать. Выбирает, какие варианты нужно отсеять в ответе: наиболее вероятные или менее. Если поставить 0.0., то вообще почти все вытащит.

   Если мы хотим увеличить вариативность ответов, то ставим температуру 1, а min_p 0.0.

   Если снизить галлюцинации, то температуру снижаем до 0.1, а min_p повышаем.

   По опыту самые оптимальные варианты - 0.1 для каждого параметра.






