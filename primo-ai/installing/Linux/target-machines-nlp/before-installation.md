# Перед установкой

В этом разделе приведены общие сведения о программных компонентах для группы целевых машин с *NLP* и системные требования к таким машинам.

## Общие сведения

Компоненты Primo.AI.Api и их связи с NLP-компонентом "AI Текст" приведены на схеме ниже:

![Компоненты Primo.AI.Api и группы целевых машин NLP](<../../../../.gitbook/assets1/primo-ai/install/nlp-components-and-machines-scheme.png>)

**Группа целевых машин NLP** - набор из 4 сервисов, обеспечивающих работу LLM-модели в рамках NLP-задач. Состоит из **Logics-сервера** с агентом и **LLM-ядра** с агентом.

**Агент Logics-сервера** – self-hosted веб-приложение, REST API. Управляет Logics-сервером, а также агентом LLM-ядра. Лицензируется (требует 1 лицензии на Агента Primo RPA AI Server). Выполнено как NET8-приложение, которое размещено на той же физической или виртуальной машине-хосте, что и контейнер **Logics-сервера**. 

**Logics-сервер** – python-приложение, логическая обвязка вокруг LLM-ядра. Размещено в отдельном docker-контейнере на той же физической или виртуальной машине-хосте, что и его **агент Logics-сервера**.

**Агент LLM-ядра** – self-hosted веб-приложение, REST API. Управляет LLM-ядром. Выполнено как NET8-приложение, которое размещено на той же физической или виртуальной машине-хосте, что и контейнер **LLM-ядра**. 

**LLM-ядро** – python-приложение, обеспечивающее работу LLM-модели. Тип ядра определяется полем **Движок** в Портале. Размещено в отдельном docker-контейнере на той же физической или виртуальной машине-хосте, что и **агент LLM-ядра**.

Группа целевых машин NLP может представлять из себя как единственную физическую или виртуальную машину с 2 docker-контейнерами и 2 агентами, так и располагаться на 2 разных машинах. При этом наибольшие требования к производительности предъявляет машина с LLM-ядром.
Не рекомендуется размещать на машине с LLM-ядром другие ресурсоёмкие приложения. 
Групп целевых машин NLP может быть много. Все группы целевых машин NLP должны быть настроены одинаково, и на каждой целевой машине должен быть развернут агент. Версии ОС на целевых машинах могут отличаться.

Порт `44392`, указанный на схеме выше, используется при настройке конфигурационных файлов агента и открытия портов на файерволе (в том числе аппаратном в сети организации). 

Для настройки группы целевых машин NLP нужна чистая машина с ОС Linux (обязательно с последними обновлениями). На машину необходимо скопировать папку с комплектом поставки. Это может быть любая папка, но для определенности пусть будет папка `/srv/samba/shared/install`*.

Для настройки группы целевых машин NLP требуется последовательно выполнить все шаги настоящего руководства.

> \**Сетевая папка, доступная с машины, на которой размещен комплект поставки.* 



## Системные требования
Для группы целевых машин NLP следует использовать рабочие станции под управлением Astra Linux, к которой предъявляются требования из таблицы ниже.

| Параметр | Машина с Logics-сервером | Машина с LLM-ядром (CPU), модели base-LLM-01/02/03 | Машина с LLM-ядром (CPU), модель base-LLM-04 | Машина с LLM-ядром (GPU) | 
| --------------- | ------ | -------------------------------- | ----------------------------------- | ----------------------------------- | 
| CPU             | 4 ядра |  2 ядра (AVX512)                 | 12 ядер (AVX512)                    | 8 ядер                              |
| RAM             | 8 Гб   |  32 Гб                           | 32 Гб                               | 32 Гб                               | 
| HDD             | 50 Гб  |  250 Гб                          | 250 Гб                              | 250 Гб                              |

{% hint style="warning" %} Примечание. Несмотря на то, что LLM-ядро с моделями base-LLM-01/02/03 запускается уже на 2 ядрах ЦПУ с AVX512, при параллельной обработке нескольких запросов время ожидания ответа при такой конфигурации будет расти драматически. {% endhint %}

{% hint style="warning" %} Примечание. Перед установкой компонентов целевой машины проверьте наличие инструкций AVX512 процессора. {% endhint %}

Проверить наличие инструкций AVX2/AVX512 можно следующей командой: 
```
grep avx /proc/cpuinfo
```
Если в выводе содержатся только AVX2-инструкции, обработка запросов моделью будет замедлена в разы, или, в зависимости от модели, невозможна.
Если в выводе отсутствуют даже AVX2-инструкции, работа с LLM-моделями будет невозможна.

## Что дальше

Выполните установку Docker на каждую целевую машину из группы:
* [Docker](https://docs.primo-rpa.ru/primo-rpa/primo-rpa-ai-server/installing/linux/installing-docker)
