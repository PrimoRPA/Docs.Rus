# AI Server 1.25.4

Список изменений для версии 1.25.4, выпущенной в апреле 2025-го года.

## Умный OCR

### Поддержка многостраничного файла документа

Стала доступна обработка многостраничных документов в форматах PDF и TIFF. Функциональность упрощает работу с большим объемом данных и минимизирует ручной труд при подготовке RPA-проектов. 

Многостраничные файлы документов поддерживаются в запросах инференса, а также на странице **Тестирование** в веб-интерфейсе AI Server. Результат классификации/распознавания будет разбит по страницам.
  
**Важно**: многостраничные документы не поддерживаются в датасетах для обучения моделей.

![](<../../release-notes/resources/ai-server/1-25-4/testingresults-manypagespdf.png>)
     
### Автоматическое определение ориентации изображения на странице Тестирование

Появилась возможность автоматически определять ориентацию изображения, которое загружается в тестовом запросе на странице **Тестирование**. В этом случае, если изображение при загрузке было повернуто боком, то сервис автоматически его повернет в нужную сторону для лучшего распознавания данных.

Чтобы использовать эту возможность, включите в шаблоне инференса параметр **Определение ориентации страницы перед распознаванием**. 


### Логи data science-ядра

Добавлены логи DS-ядра, которые помогают отслеживать проблемы. Например, если на финальной стадии запуске процесса инференса произошла перегрузка https://azure-dos.s1.primo1.orch/PrimoCollection/AI/_boards/board/t/AI%20Team/Stories/?workitem=34513

## AI Текст

### Возможность распознавать текст на изображении документа

Расширили набор LLM-моделей в поставке AI Server. Теперь вам доступна **мультимодальная модель**, которая умеет работать как с текстовой информацией, так и с изображениями документов. 

Мультимодальной модели доступны все существующие навыки, а также дополнительный навык **OCR**. Если вы назначили мультимодальной модели навык **OCR**, то она сможет распознавать текст на изображении структурированного и неструктурированного документа, в том числе рукописный текст. 

Чтобы использовать мультимодальную модель в проекте типа **AI Текст > Задачи NLP**:
* Выберите базовую модель `base-LLM-04 (Vllm, multimodal)`, движок Vllm и укажите технические характеристики устройства, на котором работает сервер LLM;
* Запустите модель на целевой машине;
* Добавьте модели навык **OCR**:

  ![](<../../release-notes/resources/ai-server/1-25-4/ocr-skill.png>)

В запросе к мультимодальной модели с навыком OCR в обязательном порядке потребуется указать файл изображения и, опционально, текст запроса. Например, в тексте запроса вы можете попросить модель найти значение какого-либо поля в паспорте.

Допустимые форматы изображений в запросах: JPG, JPEG, PNG. Ограничения по весу файла отсутствуют.

**Важно:** другие навыки мультимодальной модели не работают с изображениями.
  

### Поддержка запуска всех базовых LLM-моделей на CPU и GPU

Запуск LLM на CPU стал поддерживаться. А LLama стала запускаться на ГПУ (убрать в шаблонах модели, что не работает на гпу). Короче, теперь обе работают и на гпу, и на цпу. Кроме мультимодалки: она на ЦПУ не работает (?)

Переименовали тип проекта **С использованием LLM** в тип **AI Текст** в соответствии с названием программного компонента.




## Исправленные ошибки 

1. Исправлены незначительные ошибки локализации в веб-интерфейсе на английском и русском языках. 
1. Исправлена ошибка, когда после создания проекта типа AI Текст и переключения на вкладку Настройки или Мониторинг в верхней панели меню, некорректно отображались веб-страницы.
1. Исправлена ошибка, из-за которой некорректно отображался размер загруженного шаблона модели на странице **Настройки > Шаблоны моделей**.
1. Процесс запуск LLM-модели на целевой машине стал стабильнее (поискать задачи!)
