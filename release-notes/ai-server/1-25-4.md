# AI Server 1.25.4

Список изменений для версии 1.25.4, выпущенной в апреле 2025-го года.

## Умный OCR

### Поддержка многостраничного файла документа

Стала доступна обработка многостраничных документов в форматах PDF и TIFF. Функциональность упрощает работу с большим объемом данных и минимизирует ручной труд при подготовке RPA-проектов. 

Многостраничные файлы документов поддерживаются в запросах инференса, а также на странице **Тестирование** в веб-интерфейсе AI Server. Результат классификации/распознавания будет разбит по страницам.
  
**Важно**: многостраничные документы не поддерживаются в датасетах для обучения моделей.

![](<../../release-notes/resources/ai-server/1-25-4/testingresults-manypagespdf.png>)
     
### Автоматическое определение ориентации изображения в тестовых запросах

Появилась возможность автоматически определять ориентацию изображения, которое отправляется в тестовом запросе на странице **Тестирование**. В этом случае, если изображение при загрузке было повернуто боком, то сервис автоматически его повернет в нужную сторону для лучшего распознавания данных.

Чтобы использовать эту возможность, включите в шаблоне инференса параметр **Определение ориентации страницы перед распознаванием**. 


### Просмотр логов ядра Data Science

На страницах всех типов процессов (обучение/инференс/авторазметка) появилась возможность просматривать логи ядра Data Science. Опция позволяет отслеживать возможные проблемы при запуске процессов и доступна в меню действий процесса. Список логов возможно скопировать.

![](<../../release-notes/resources/ai-server/1-25-4/processes-action-logs.png>)



## AI Текст

### Возможность распознавать текст на изображении документа

Расширили набор LLM-моделей в поставке AI Server — теперь вам доступна **мультимодальная модель**, которая умеет работать как с текстовой информацией, так и с изображениями документов. 

Мультимодальная модель поддерживает все существующие навыки, а также дополнительный навык **OCR**. Если вы назначили мультимодальной модели навык **OCR**, то она сможет распознавать текст на изображении структурированного и неструктурированного документа, в том числе рукописный текст. 

Чтобы использовать мультимодальную модель в проекте типа **AI Текст > Задачи NLP**:
* Выберите базовую модель `base-LLM-04 (Vllm, multimodal)`, движок Vllm и укажите технические характеристики устройства, на котором работает сервер LLM;
* Запустите модель на целевой машине;
* Добавьте модели навык **OCR**:

  ![](<../../release-notes/resources/ai-server/1-25-4/ocr-skill.png>)

В запросе к мультимодальной модели с навыком OCR в обязательном порядке потребуется указать файл изображения и, опционально, текст запроса. Например, в тексте запроса вы можете попросить модель найти значение какого-либо поля в паспорте.

Допустимые форматы изображений в запросах: JPG, JPEG, PNG. Ограничения по весу файла отсутствуют.

**Важно:** другие навыки мультимодальной модели не работают с изображениями.
  

### Поддержка работы всех базовых LLM-моделей на CPU и GPU

* Базовые модели с движком Vllm теперь возможно запустить на CPU;
* Базовые модели с движком Llama стало возможным запускать на GPU;
* Мультимодальная модель запускается на GPU и CPU. Требования к CPU: поддержка расширений AVX-512, оперативная память > 24 Гб.


### Улучшения 

Переименовали тип проекта **С использованием LLM** в тип **AI Текст** в соответствии с названием программного компонента.




## Исправленные ошибки 

1. Исправлены незначительные ошибки локализации в веб-интерфейсе на английском и русском языках. 
1. Исправлена ошибка, из-за которой некорректно отображался размер загруженного шаблона модели на странице **Настройки > Шаблоны моделей**.
1. Процесс запуск LLM-модели на целевой машине стал стабильнее (поискать задачи!)
