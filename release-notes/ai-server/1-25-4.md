# AI Server 1.25.4

Список изменений для версии 1.25.4, выпущенной в апреле 2025-го года.

## Умный OCR

### Поддержка многостраничного файла документа

Стала доступна обработка многостраничных документов в форматах PDF и TIFF. Функциональность упрощает работу с большим объемом данных и минимизирует ручной труд при подготовке RPA-проектов. 

Многостраничные файлы документов поддерживаются в запросах инференса, а также на странице **Тестирование** в веб-интерфейсе AI Server. Результат классификации/распознавания будет разбит по страницам.
  
**Важно**: многостраничные документы не поддерживаются в датасетах для обучения моделей.

![](<../../release-notes/resources/ai-server/1-25-4/testingresults-manypagespdf.png>)
     
### Исправление ориентации изображения в тестовых запросах

Появилась возможность автоматически определять ориентацию изображения в тестовом запросе на странице **Тестирование**. В случае, если изображение имело неправильную ориентацию (например, повернуто набок), то сервис автоматически это исправит для лучшего распознавания данных.

Чтобы использовать эту возможность, включите в шаблоне инференса параметр **Определение ориентации страницы перед распознаванием**. 


### Просмотр логов ядра Data Science

На страницах всех типов процессов (обучение/инференс/авторазметка) появилась возможность просматривать логи ядра Data Science. Опция позволяет отслеживать возможные проблемы при запуске процессов и доступна в меню действий процесса. Список логов возможно скопировать.

![](<../../release-notes/resources/ai-server/1-25-4/processes-action-logs.png>)



## AI Текст

### Распознавание текста на изображении документа

Расширили набор LLM-моделей в поставке AI Server. Теперь вам доступна **мультимодальная модель**, которая умеет работать как с текстовой информацией, так и с изображениями документов. 

Мультимодальная модель поддерживает все существующие навыки, а также дополнительный навык **OCR**. Если вы назначили модели навык **OCR**, то она сможет распознавать текст на изображении структурированного и неструктурированного документа, в том числе рукописный текст. 

Чтобы использовать мультимодальную модель в проекте типа **AI Текст > Задачи NLP**:
* Выберите базовую модель `base-LLM-04 (Vllm, multimodal)`, движок Vllm и укажите технические характеристики устройства, на котором работает сервер LLM;
* Запустите модель на целевой машине;
* Добавьте модели навык **OCR**:

  ![](<../../release-notes/resources/ai-server/1-25-4/ocr-skill.png>)

В запросе к мультимодальной модели с навыком OCR требуется обязательно указать файл изображения и, опционально, текст запроса. Например, в тексте запроса вы можете попросить модель найти значение какого-либо поля в паспорте.

Допустимые форматы изображений в запросе: JPG, JPEG, PNG. Ограничения по весу файла отсутствуют.

**Важно:** другие навыки мультимодальной модели не работают с изображениями.
  

### Улучшения

1. Все базовые LLM-модели стали поддерживать работу на CPU и GPU:
   * Базовые модели с движком Vllm теперь запускаются на CPU;
   * Базовые модели с движком Llama стали запускаться на GPU;
   * Мультимодальная модель запускается на GPU и CPU, который соответствует требованиям: 12 физических ядер, поддержка расширений AVX-512, оперативная память > 24 Гб.
1. Переименовали тип проекта **С использованием LLM** в тип **AI Текст** в соответствии с названием программного компонента.




## Исправленные ошибки 

1. Исправлены незначительные ошибки локализации в веб-интерфейсе на английском и русском языках. 
1. Исправлена ошибка, из-за которой некорректно отображался размер загруженного шаблона модели на странице **Настройки > Шаблоны моделей**.
1. Процесс запуск LLM-модели на целевой машине стал стабильнее (поискать задачи!)
